{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Some useful manuals to start Spark in Colab:\n",
        "*   https://levelup.gitconnected.com/launch-spark-on-google-colab-and-connect-to-sparkui-342cad19b304\n",
        "*   https://medium.com/@TheITspace/running-pyspark-on-google-colab-2552435972b3\n",
        "*   https://colab.research.google.com/drive/1fa2G3YuXx3Isqyby5kFETqmWotFwtqlH?usp=sharing#scrollTo=-JgkMmYgS0Za\n",
        "\n",
        "For accessing web ui from outside of Colab we may use ngrok: https://ngrok.com/docs/getting-started/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TPnqDPhqHEkp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "74y_myeLDfOl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "        .appName('myColabSpark') \\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "_33U-n0OIfXe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok, conf\n",
        "import getpass\n",
        "\n",
        "print(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "conf.get_default().auth_token = getpass.getpass()\n",
        "\n",
        "tunnel = ngrok.connect(addr=spark.sparkContext.uiWebUrl, bind_tls=True)\n",
        "print(f\" * ngrok tunnel \\\"{tunnel.public_url}\\\" -> \\\"{spark.sparkContext.uiWebUrl}\\\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoCR3xlMEb3m",
        "outputId": "e274f2a5-8476-4779-8ee4-47ee248e5bfb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken\n",
            "··········\n",
            " * ngrok tunnel \"https://d557-34-138-57-80.ngrok-free.app\" -> \"http://4b5d4d978470:4040\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "\n",
        "file_url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet'\n",
        "\n",
        "spark.sparkContext.addFile(file_url)\n",
        "\n",
        "df = spark.read.parquet(SparkFiles.get('yellow_tripdata_2024-10.parquet'), header=True)\n",
        "\n",
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQDClamEEexD",
        "outputId": "3626a129-f31a-46a7-e0dd-2b40ddf279fa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3833771"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1"
      ],
      "metadata": {
        "id": "Mda0ntIaFS3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "I37gR-TREiRB",
        "outputId": "f0aa6e17-db74-4d73-ffda-877bb1445b8a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.5.5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2"
      ],
      "metadata": {
        "id": "Uax1foevFVBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir_to_save_parquets = 'yellow_tripdata_2024-10_parquets'\n",
        "df = df.repartition(4)\n",
        "df.write.parquet(dir_to_save_parquets)"
      ],
      "metadata": {
        "id": "WFKiKP35FWti"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "files = os.listdir(dir_to_save_parquets)\n",
        "print('Saved files:')\n",
        "print('\\n'.join(files))\n",
        "\n",
        "parquet_files = [f for f in files if f.endswith('.parquet')]\n",
        "parquet_file_sizes_in_bytes = [(f, os.path.getsize(os.path.join(dir_to_save_parquets, f))) for f in parquet_files]\n",
        "parquet_file_sizes_in_MB = [(f, size / (1024 ** 2)) for f, size in parquet_file_sizes_in_bytes]\n",
        "\n",
        "print('\\nParquets sizes:')\n",
        "for file, size in parquet_file_sizes_in_MB:\n",
        "    print(f\"File: {file}, Size: {size:.2f} MB\")\n",
        "\n",
        "average_in_bytes = sum([size for _,size in parquet_file_sizes_in_bytes]) / len(parquet_files) if parquet_files else 0\n",
        "print('\\nAvg size of parquet file (Mb):', average_in_bytes/(1024**2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53eSHJfqFiRJ",
        "outputId": "a5dc4391-4b61-4cb1-b092-635b5e2c0022"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved files:\n",
            ".part-00001-fe83042f-56bf-497a-a1ff-7aed32de19bf-c000.snappy.parquet.crc\n",
            "part-00001-fe83042f-56bf-497a-a1ff-7aed32de19bf-c000.snappy.parquet\n",
            ".part-00002-fe83042f-56bf-497a-a1ff-7aed32de19bf-c000.snappy.parquet.crc\n",
            "._SUCCESS.crc\n",
            "_SUCCESS\n",
            "part-00003-fe83042f-56bf-497a-a1ff-7aed32de19bf-c000.snappy.parquet\n",
            "part-00002-fe83042f-56bf-497a-a1ff-7aed32de19bf-c000.snappy.parquet\n",
            "part-00000-fe83042f-56bf-497a-a1ff-7aed32de19bf-c000.snappy.parquet\n",
            ".part-00000-fe83042f-56bf-497a-a1ff-7aed32de19bf-c000.snappy.parquet.crc\n",
            ".part-00003-fe83042f-56bf-497a-a1ff-7aed32de19bf-c000.snappy.parquet.crc\n",
            "\n",
            "Parquets sizes:\n",
            "File: part-00001-fe83042f-56bf-497a-a1ff-7aed32de19bf-c000.snappy.parquet, Size: 23.02 MB\n",
            "File: part-00003-fe83042f-56bf-497a-a1ff-7aed32de19bf-c000.snappy.parquet, Size: 23.05 MB\n",
            "File: part-00002-fe83042f-56bf-497a-a1ff-7aed32de19bf-c000.snappy.parquet, Size: 23.06 MB\n",
            "File: part-00000-fe83042f-56bf-497a-a1ff-7aed32de19bf-c000.snappy.parquet, Size: 23.04 MB\n",
            "\n",
            "Avg size of parquet file (Mb): 23.042235136032104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3"
      ],
      "metadata": {
        "id": "KMSBMg9pFxD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9KjVUl4Fy3s",
        "outputId": "13a7d754-d549-42eb-ac97-3c027de2a136"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- VendorID: integer (nullable = true)\n",
            " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
            " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
            " |-- passenger_count: long (nullable = true)\n",
            " |-- trip_distance: double (nullable = true)\n",
            " |-- RatecodeID: long (nullable = true)\n",
            " |-- store_and_fwd_flag: string (nullable = true)\n",
            " |-- PULocationID: integer (nullable = true)\n",
            " |-- DOLocationID: integer (nullable = true)\n",
            " |-- payment_type: long (nullable = true)\n",
            " |-- fare_amount: double (nullable = true)\n",
            " |-- extra: double (nullable = true)\n",
            " |-- mta_tax: double (nullable = true)\n",
            " |-- tip_amount: double (nullable = true)\n",
            " |-- tolls_amount: double (nullable = true)\n",
            " |-- improvement_surcharge: double (nullable = true)\n",
            " |-- total_amount: double (nullable = true)\n",
            " |-- congestion_surcharge: double (nullable = true)\n",
            " |-- Airport_fee: double (nullable = true)\n",
            " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
            " |-- time_diff_seconds: long (nullable = true)\n",
            " |-- time_diff_hours: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, to_timestamp\n",
        "\n",
        "df = df.withColumn(\"lpep_pickup_datetime\", to_timestamp(col=\"tpep_pickup_datetime\", format=\"yyyy-MM-dd HH:mm:ss\"))\n",
        "df = df.withColumn(\"tpep_dropoff_datetime\", to_timestamp(col=\"tpep_dropoff_datetime\", format=\"yyyy-MM-dd HH:mm:ss\"))\n",
        "\n",
        "filtered_df = df.filter(\n",
        "    (col(\"tpep_pickup_datetime\") >= \"2024-10-15 00:00:00\") &\n",
        "    (col(\"tpep_pickup_datetime\") < \"2024-10-16 00:00:00\")\n",
        ")\n",
        "# filtered_df = df.filter(col(\"lpep_pickup_datetime\").between(\"2024-10-15 00:00:00\", \"2024-10-15 23:59:59\"))\n",
        "\n",
        "print(f\"15th of October trips: {filtered_df.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-e1fDbbF44z",
        "outputId": "6a875a81-724c-4d5b-8db1-abd8f59221ac"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15th of October trips: 128893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4"
      ],
      "metadata": {
        "id": "Afsk54xAF8Av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, unix_timestamp, expr\n",
        "\n",
        "df = df.withColumn(\"time_diff_seconds\",\n",
        "                   unix_timestamp(col(\"tpep_dropoff_datetime\")) - unix_timestamp(col(\"lpep_pickup_datetime\")))\n",
        "\n",
        "df = df.withColumn(\"time_diff_hours\", expr(\"time_diff_seconds / 3600\"))\n",
        "\n",
        "longest_trip_in_hours = df.orderBy(col(\"time_diff_hours\").desc()).first()\n",
        "\n",
        "print(f\"Longest trip in hours: {longest_trip_in_hours['time_diff_hours']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a1wpYK0F8sx",
        "outputId": "20bd7d65-4f5d-4699-cc8d-5892f6464923"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longest trip in hours: 162.62 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5"
      ],
      "metadata": {
        "id": "yu5MUvtjGDHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Local spark url (with port) is: {spark.sparkContext.uiWebUrl}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lR889-VrGCiw",
        "outputId": "2f10c00c-541d-42f3-b0fe-244573a3e4fd"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local spark url (with port) is: http://4b5d4d978470:4040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6"
      ],
      "metadata": {
        "id": "Ns4yM8mcHbtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_zones_url = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv'\n",
        "spark.sparkContext.addFile(file_zones_url)\n",
        "\n",
        "zone_df = spark.read.csv(SparkFiles.get(os.path.basename(file_zones_url)),\n",
        "                         header=True,\n",
        "                         inferSchema=True)\n",
        "\n",
        "zone_df.printSchema()\n",
        "\n",
        "zone_df.show(10, truncate=False)\n",
        "\n",
        "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# schema = StructType([\n",
        "#     StructField(\"LocationID\", IntegerType(), True),\n",
        "#     StructField(\"Borough\", StringType(), True),\n",
        "#     StructField(\"Zone\", StringType(), True),\n",
        "#     StructField(\"service_zone\", StringType(), True)\n",
        "# ])\n",
        "# zone_df = spark.read.csv(SparkFiles.get('taxi_zone_lookup.csv'),\n",
        "#                          header=True,\n",
        "#                          schema=schema)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lok6wtWHh0O",
        "outputId": "2ba0d97f-ec92-40c2-cd64-cf30fdb57673"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- LocationID: integer (nullable = true)\n",
            " |-- Borough: string (nullable = true)\n",
            " |-- Zone: string (nullable = true)\n",
            " |-- service_zone: string (nullable = true)\n",
            "\n",
            "+----------+-------------+-----------------------+------------+\n",
            "|LocationID|Borough      |Zone                   |service_zone|\n",
            "+----------+-------------+-----------------------+------------+\n",
            "|1         |EWR          |Newark Airport         |EWR         |\n",
            "|2         |Queens       |Jamaica Bay            |Boro Zone   |\n",
            "|3         |Bronx        |Allerton/Pelham Gardens|Boro Zone   |\n",
            "|4         |Manhattan    |Alphabet City          |Yellow Zone |\n",
            "|5         |Staten Island|Arden Heights          |Boro Zone   |\n",
            "|6         |Staten Island|Arrochar/Fort Wadsworth|Boro Zone   |\n",
            "|7         |Queens       |Astoria                |Boro Zone   |\n",
            "|8         |Queens       |Astoria Park           |Boro Zone   |\n",
            "|9         |Queens       |Auburndale             |Boro Zone   |\n",
            "|10        |Queens       |Baisley Park           |Boro Zone   |\n",
            "+----------+-------------+-----------------------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zone_df.createOrReplaceTempView(\"zone_table\")\n",
        "df.createOrReplaceTempView(\"yellow_tripdata\")\n",
        "\n",
        "query_str = \"\"\"\n",
        "    SELECT\n",
        "        zones.Zone,\n",
        "        COUNT(*) AS trip_count\n",
        "    FROM\n",
        "        yellow_tripdata AS yt\n",
        "    INNER JOIN\n",
        "        zone_table AS zones\n",
        "    ON\n",
        "        yt.PULocationID = zones.LocationID\n",
        "    GROUP BY\n",
        "        zones.Zone\n",
        "    ORDER BY\n",
        "        trip_count ASC\n",
        "    LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "least_frequent_zone = spark.sql(query_str)\n",
        "\n",
        "least_frequent_zone.show(1, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ3Wxb_CHkts",
        "outputId": "59265a0b-c5e8-416a-840c-8be7f2c6f8f7"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------+----------+\n",
            "|Zone                                         |trip_count|\n",
            "+---------------------------------------------+----------+\n",
            "|Governor's Island/Ellis Island/Liberty Island|1         |\n",
            "+---------------------------------------------+----------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    }
  ]
}